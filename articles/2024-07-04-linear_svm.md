---
title: "SVMã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè£…ã™ã‚‹"
emoji: "ğŸ’¨"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python", "SVM", "æ©Ÿæ¢°å­¦ç¿’", "Tech", "æ•°ç†æœ€é©åŒ–"]
published: false
---

## TL;DR
- SVMã‚’å®Ÿè£…ã—ã¦ã„ã‚‹è¨˜äº‹ã¯ã‚ã¾ã‚Šè¦‹ã‹ã‘ãªã‹ã£ãŸã®ã§pythonã§å®Ÿè£…ã—ã¦ã¿ã¾ã—ãŸï¼
- SVMã®ä¸»å•é¡Œã‚’ï¼ŒL2æ­£å‰‡åŒ–çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–å•é¡Œã¨ã—ã¦å‹¾é…é™ä¸‹æ³•ã§è§£ãã¾ã™ï¼
- è¨ˆç®—é€Ÿåº¦ã¯scikit-learnã®SVCã®ã‚ˆã†ãªå·¥å¤«ã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯åŠã³ã¾ã›ã‚“ï¼

## æœ¬è¨˜äº‹ãŒå®Ÿè£…ã™ã‚‹SVM
2ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è¡Œã†ï¼Œã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã®å®Ÿè£…ã‚’è¡Œã„ã¾ã™ï¼
ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’å–ã‚Šå…¥ã‚ŒãŸã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã¯æ¬¡å›å®Ÿè£…ã—ã¾ã™ï¼

## SVMã®ã–ã£ãã‚Šã¨ã—ãŸç†è«–
<!-- $n$å€‹ã®ç‰¹å¾´é‡$\bm{x}_i \in \mathbb{R}^d$ã¨ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«$y_i \in \{-1, 1\}$ -->
ç‰¹å¾´é‡$\bm{x} \in \mathbb{R}^d$ã¨é‡ã¿$\bm{w} \in \mathbb{R}^d$ã¨ãƒã‚¤ã‚¢ã‚¹$b \in \mathbb{R}$ã«å¯¾ã—ã¦ï¼Œæ¬¡ã®ã‚ˆã†ãª1æ¬¡é–¢æ•°ã‚’è€ƒãˆã¾ã™ï¼
$$f(\bm{x}) = \bm{w}^\top \bm{x} + b$$

ã“ã®1æ¬¡é–¢æ•°ã«å¯¾ã—ã¦ï¼Œã‚¯ãƒ©ã‚¹ãŒ$-1, +1$ã®ã©ã¡ã‚‰ã‹ã§ã‚ã‚‹ã¨ã—ï¼Œ$f(\bm{x}) > 0$ã§ã‚ã‚‹ã¨ãï¼Œ$\bm{x}$ã®ã‚¯ãƒ©ã‚¹ã‚’$+1$ã¨äºˆæ¸¬ã—ï¼Œ$f(\bm{x}) < 0$ã§ã‚ã‚‹ã¨ãï¼Œ$\bm{x}$ã®ã‚¯ãƒ©ã‚¹ã‚’$-1$ã¨äºˆæ¸¬ã™ã‚‹ã¨ã—ã¾ã™ï¼


ã“ã“ã§ï¼Œã‚ˆã‚Šäºˆæ¸¬ç²¾åº¦ã®é«˜ã„1æ¬¡é–¢æ•°ã‚’å¼•ãã«ã¯ï¼Œé‡ã¿$\bm{w}$ã¨ãƒã‚¤ã‚¢ã‚¹$b$ã‚’ã©ã®ã‚ˆã†ã«æ±ºã‚ã‚Œã°è‰¯ã„ã§ã—ã‚‡ã†ã‹ï¼

ä¸€èˆ¬ã«ã‚¯ãƒ©ã‚¹ã‚’åˆ†é¡ã™ã‚‹å¢ƒç•Œ$f(\bm{x}) = 0$ã‚’**åˆ†é¡å¢ƒç•Œ**ã¨å‘¼ã³ï¼Œ
åˆ†é¡å¢ƒç•Œã‚’æŒŸã‚“ã§2ã¤ã®ã‚¯ãƒ©ã‚¹ãŒã©ã‚Œãã‚‰ã„é›¢ã‚Œã¦ã„ã‚‹ã‹ã‚’**ãƒãƒ¼ã‚¸ãƒ³**ã¨å‘¼ã³ã¾ã™ï¼

[SVM(ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³)](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3)ã¯ã“ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ï¼Œã‚ˆã‚Šäºˆæ¸¬ç²¾åº¦ã®é«˜ã„1æ¬¡é–¢æ•°ã‚’æ±‚ã‚ã¾ã™ï¼

ã•ã¦ï¼Œä»Šï¼Œç°¡å˜ã®ãŸã‚ã«ç‰¹å¾´é‡$\bm{x}$ã¨é‡ã¿$\bm{w}$ã‚’æ¬¡ã®ã‚ˆã†ã«æ‹¡å¼µã—ã¾ã™ï¼
$$\tilde{\bm{x}} = [\bm{x}, 1]^\top, \tilde{\bm{w}} = [\bm{w}, b]$$
ã™ã‚‹ã¨ï¼Œè­˜åˆ¥å¢ƒç•Œã¯
$$f(\bm{x}) = \bm{w}^\top \bm{x} + b = \tilde{\bm{w}}^\top \tilde{\bm{x}}$$
ã¨ç°¡æ½”ã«è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ï¼

**ä»¥ä¸‹ã§ã¯å˜ã«$\tilde{\bm{x}}$ã‚’$\bm{x}$ï¼Œ$\tilde{\bm{w}}$ã‚’$\bm{w}$ã¨è¡¨è¨˜ã—ã¾ã™ï¼**



### ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³SVMã®ä¸»å•é¡Œ
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚‹1æ¬¡é–¢æ•°ã§ã‚¯ãƒ©ã‚¹ã‚’å®Œå…¨ã«åˆ†é›¢ã§ãã‚‹ã¨ãï¼Œãã‚Œã‚’[ç·šå½¢åˆ†é›¢å¯èƒ½](https://ja.wikipedia.org/wiki/%E7%B7%9A%E5%BD%A2%E5%88%86%E9%9B%A2%E5%8F%AF%E8%83%BD)ã§ã‚ã‚‹ã¨ã„ã„ã¾ã™ï¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’**ç·šå½¢åˆ†é›¢å¯èƒ½ã§ã‚ã‚‹ã¨ä»®å®šã—ãŸã¨ãã®SVM**ã‚’[ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³SVM](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3#%E3%83%8F%E3%83%BC%E3%83%89%E3%83%9E%E3%83%BC%E3%82%B8%E3%83%B3)ã¨å‘¼ã³ã¾ã™ï¼

ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³SVMã®ä¸»å•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã›ã¾ã™ï¼
$$\min_w{||\bm{w}||^2} \quad \text{s.t.} \quad y_i (\bm{w}^\top \bm{x}_i) \ge 1$$

### ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã®ä¸»å•é¡Œ
ä¸€èˆ¬ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç·šå½¢åˆ†é›¢å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã¯å°‘ãªã„ã§ã™ï¼ãã®ãŸã‚ï¼Œ$\xi$ã ã‘åˆ†é¡å¢ƒç•Œã‹ã‚‰ã®èª¤å·®ã‚’è¨±å®¹ã™ã‚‹ã‚ˆã†ã«ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³SVMã‚’ä¸€èˆ¬åŒ–ã—ãŸæ‰‹æ³•ã‚’[ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVM](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3#%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%BC%E3%82%B8%E3%83%B3)ã¨å‘¼ã³ã¾ã™ï¼

å®šæ•°$C \in \mathbb{R}$ã¨ã—ã¦ï¼Œã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã®ä¸»å•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã›ã¾ã™ï¼
$$\min_w{\frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\xi_i} \quad \text{s.t.} \quad \xi_i \ge 0, \quad \bm{y}_i(\bm{w}^\top\bm{x_i}) \ge 1 - \xi_i$$

### ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã¨çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–
**ã“ã“ã‹ã‚‰ä¸€èˆ¬çš„ã«SVMã‚’è§£ããŸã‚ã«ç”¨ã„ã‚‰ã‚Œã‚‹æ‰‹æ³•ã¨ã¯é•ã†ã‚‚ã®ã«ãªã‚Šã¾ã™ï¼**

äººå·¥å¤‰æ•°$\xi \in \mathbb{R}$ã«å¯¾ã—ã¦ï¼Œä»»æ„ã®å˜èª¿å¢—åŠ ãªé–¢æ•°$g: \mathbb{R} \rightarrow \mathbb{R}$ã§æ¬¡ã®2ã¤ã¯ç­‰ä¾¡ã«ãªã‚Šã¾ã™ï¼
$$\min_z{g(\max\{0, 1-z\})} \iff \min_{z, \xi}{g(\xi)} \quad \text{s.t.} \quad \xi \ge 0 \quad z \ge 1 - \xi$$

ã“ã®åŒå€¤é–¢ä¿‚ã‹ã‚‰ï¼Œã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã®ä¸»å•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼
$$\begin{gather*}\min_w{R(\bm{w})}\\ \text{where} \quad R(\bm{w}) := \frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\max\{0, 1-y_i(\bm{w}^\top \bm{x_i})\}\end{gather*}$$


ã“ã“ã§ï¼Œ$\max\{0, 1-y_i(\bm{w}^\top \bm{x_i})\}$ã¯[ãƒ’ãƒ³ã‚¸æå¤±](https://en.wikipedia.org/wiki/Hinge_loss)ã¨å‘¼ã°ã‚Œã‚‹[æå¤±é–¢æ•°](https://ja.wikipedia.org/wiki/%E6%90%8D%E5%A4%B1%E9%96%A2%E6%95%B0)ã§ã™ï¼
ã™ã‚‹ã¨ï¼Œã“ã®å¼ã¯ï¼Œæ·±å±¤å­¦ç¿’ã§ã‚‚ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹æ­£å‰‡åŒ–[çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–(Empirical risk minimazation)](https://en.wikipedia.org/wiki/Empirical_risk_minimization)ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã¨ä¸€è‡´ã—ã¾ã™ï¼

### ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã¨æœ€æ€¥é™ä¸‹æ³•
ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã®æœ€å°åŒ–å•é¡Œã‚’è§£ãæ–¹æ³•ã®ä¸€ã¤ã¨ã—ã¦ï¼ŒçµŒé¨“ãƒªã‚¹ã‚¯$R$ãŒ[å‡¸é–¢æ•°](https://ja.wikipedia.org/wiki/%E5%87%B8%E9%96%A2%E6%95%B0)ã§ã‚ã‚‹ã“ã¨ã‹ã‚‰ï¼Œå˜ç´”ã«[æœ€æ€¥é™ä¸‹æ³•](https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95)ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æœ€é©è§£ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼
ã¤ã¾ã‚Šï¼Œ$t$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã®é‡ã¿$\bm{w}^{(t)}$ã¯ï¼Œå­¦ç¿’ç‡$\eta \in \mathbb{R}$ã‚’å®šã‚ã¦ï¼Œå‰ã‚¹ãƒ†ãƒƒãƒ—ã®é‡ã¿$\bm{w}^{(t-1)}$ã¨ï¼ŒçµŒé¨“ãƒªã‚¹ã‚¯$R$ã®å‹¾é…ã‹ã‚‰ï¼Œæ¬¡ã®ã‚ˆã†ã«æ±ºã‚ã¦ã„ã‘ã°è‰¯ã„ã“ã¨ã«ãªã‚Šã¾ã™ï¼
$$\bm{w}^{(t)} = \bm{w}^{(t-1)} - \eta \nabla R(\bm{w}^{(t-1)})$$

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ååˆ†ã«ç¹°ã‚Šè¿”ã™ã“ã¨ã§ï¼Œé‡ã¿$\bm{w}$ã¯çµŒé¨“ãƒªã‚¹ã‚¯$R$ã‚’æœ€å°ã¨ã™ã‚‹æœ€é©è§£ã«è¿‘ã¥ã„ã¦ã„ãã¾ã™ï¼

ã“ã“ã§ï¼ŒçµŒé¨“ãƒªã‚¹ã‚¯$R$ã®(åŠ£)å‹¾é…ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼
$$\nabla R(\bm{w}) = \lambda \bm{w} + \frac{1}{n} \sum^n_{i=1}\begin{cases} -y_i\bm{x_i} &\text{if } 1-y_i(\bm{w}^\top \bm{x_i}) \ge 0 \\ 0 &\text{otherwise}\end{cases}$$

## ç·šå½¢SVMã®å®Ÿè£…
### å˜ç´”ãªå®Ÿè£…
ã•ã¦ï¼Œå®Ÿè£…ã¨ã—ã¦ã¯ï¼Œ**ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã¨æœ€æ€¥é™ä¸‹æ³•**ã®éƒ¨åˆ†ã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«æ›¸ãèµ·ã“ã›ã°è‰¯ã„ã“ã¨ã«ãªã‚Šã¾ã™ï¼

ã¾ãšï¼Œæ¬¡ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ã¾ã—ãŸï¼
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs


np.random.seed(42)

N = 100
n_features = 2
X, y = make_blobs(random_state=8,
                  n_samples=N,
                  n_features=n_features,
                  cluster_std=3,
                  centers=2)

y = y * 2 - 1 # change y_i {0, 1} to {-1, 1}

plt.figure(figsize=(8, 7))
plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.plot()
```
![](/images/2024-07-04-linear_svm/dataset1.png)

æ¬¡ã«ï¼Œã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã‚’å®Ÿè£…ã—ã¾ã™ï¼
```python
import numpy.linalg as LA


X = np.c_[X, np.ones(X.shape[0])] # set bias

# regularization parameter
lam = 0.01

# empirical risk
R = lambda w_t: 0.5 * lam * LA.norm(w_t, ord=2)**2 + np.where(1 - y * (X @ w_t) >= 0, 1 - y * (X @ w_t), 0).mean()

# gradient of empirical risk
dR = lambda w_t: lam * w_t + ((np.where(1 - y * (X @ w) >= 0, 1, 0) * -y).reshape(-1, 1) * X).mean(axis=0)

# learning rate
learning_rate = 0.01

# amount of iteration
n_iters = 5000

# weights
w = np.ones((X.shape[1]))

# gradient descent
for i in range(n_iters):
    w = w - learning_rate * dR(w)
    if (i + 1) % 1000 == 0:
        print(f"{i+1:4}: R(w) = {R(w)}")

print(f"weights: {w}")
# 1000: R(w) = 0.2580498763840658
# 2000: R(w) = 0.2518440922366277
# 3000: R(w) = 0.25032312599721734
# 4000: R(w) = 0.24936754026280172
# 5000: R(w) = 0.24909738755742183
# weights: [ 0.08554331 -0.4618898   1.69628161]
```

æ¬¡ã®ã‚ˆã†ã«ã‚¯ãƒ©ã‚¹ã‚’åˆ†é›¢ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼
```python
plt.figure(figsize=(8, 7))

plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')

# w0*x + w1*y + w2 = 0
# y = - (w0*x + w2) / w1
plt.plot(X[:, 0], - (w[0] * X[:, 0] + w[2]) / w[1])
```
![plot_boundary1](/images/2024-07-04-linear_svm/plot_boundary1.png)

### sklearn-APIã«è¿‘ã„å®Ÿè£…
å˜ç´”ãªå®Ÿè£…ã§ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ†ã‹ã‚Œã¦ãŠã‚‰ãšï¼Œæ©Ÿæ¢°å­¦ç¿’ã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ä¸ååˆ†ã§ã™ï¼
ã‚ˆã‚Šå®Ÿç”¨çš„ã§ã‚ã‚‹ï¼Œscikit-learnã®APIã«è¿‘ã„å®Ÿè£…ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼
```python
import numpy as np
import numpy.linalg as LA


class LinearSVM:
    def __init__(
        self,
        lam: float = 0.01,
        n_iters: int = 5000,
        learning_rate: float = 0.01,
        bias=True,
        verbose=False,
    ):
        self.lam = lam
        self.n_iters = n_iters
        self.learning_rate = learning_rate
        self.bias = bias
        self.verbose = verbose
        self._w = None

    def _empirical_risk(self, X: np.ndarray, y: np.ndarray, w_t: np.ndarray) -> np.ndarray:
        regularzation = 0.5 * self.lam * LA.norm(w_t, ord=2) ** 2
        loss = np.where(1 - y * (X @ w_t) >= 0, 1 - y * (X @ w_t), 0).mean()
        return regularzation + loss

    def _empirical_risk_grad(self, X: np.ndarray, y: np.ndarray, w_t: np.ndarray) -> np.ndarray:
        regularzation_grad = self.lam * w_t
        loss_grad = ((np.where(1 - y * (X @ w_t) >= 0, 1, 0) * -y).reshape(-1, 1) * X).mean(axis=0)
        return regularzation_grad + loss_grad

    def fit(self, X: np.ndarray, y: np.ndarray):
        if self.bias:
            X = np.c_[X, np.ones(X.shape[0])]

        # weights
        w = np.ones((X.shape[1]))

        # gradient descent
        for i in range(self.n_iters):
            w = w - self.learning_rate * self._empirical_risk_grad(X, y, w)
            if self.verbose and (i + 1) % 1000 == 0:
                print(f"{i+1:4}: R(w) = {self._empirical_risk(X, y, w)}")
        self._w = w

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        if self.bias:
            X = np.c_[X, np.ones(X.shape[0])]
        return X @ self._w

    def predict(self, X: np.ndarray) -> np.ndarray:
        scores = self.decision_function(X)
        return np.where(scores > 0, 1, -1)
```

å®Ÿéš›ã«ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼
```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


np.random.seed(42)

N = 500
n_features = 2
X, y = make_blobs(random_state=8,
                  n_samples=N,
                  n_features=n_features,
                  cluster_std=3,
                  centers=2)

y = y * 2 - 1 # change y_i {0, 1} to {-1, 1}

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LinearSVM()
model.fit(X_train, y_train)
preds = model.predict(X_test)

print(f"ACC: {accuracy_score(y_test, preds)}")
# ACC: 0.936

plt.figure(figsize=(8, 7))

plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')

# w0*x + w1*y + w2 = 0
# y = - (w0*x + w2) / w1
plt.plot(X[:, 0], - (w[0] * X[:, 0] + w[2]) / w[1])
```
![plot_boundary2](/images/2024-07-04-linear_svm/plot_boundary2.png)

æ­£è§£ç‡ãŒ```ACC: 0.936```ã¨ãªã‚Šï¼Œã†ã¾ãäºˆæƒ³ã§ãã¦ã„ã¾ã™ï¼

## ç·šå½¢SVMã®é™ç•Œ
ä¾‹ãˆã°æ¬¡ã®ã‚ˆã†ãªmoonãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äºˆæ¸¬ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼
```python
from sklearn.datasets import make_moons

N = 500
n_features = 2
X, y = make_moons(n_samples=N, noise=0.1, random_state=1)
y = y * 2 - 1 # change y_i {0, 1} to {-1, 1}
X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LinearSVM()
model.fit(X_train, y_train)
preds = model.predict(X_test)

print(f"ACC: {accuracy_score(y_test, preds)}")
# ACC: 0.896

plt.figure(figsize=(8, 7))
plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')
w = model._w
plt.plot(X[:, 0], - (w[0] * X[:, 0] + w[2]) / w[1])
```
![dataset_moon](/images/2024-07-04-linear_svm/dataset_moon.png)
æ­£è§£ç‡ã¯```ACC: 0.896```ã¨ãã“ã¾ã§æ‚ªãã¯ã‚ã‚Šã¾ã›ã‚“ãŒï¼Œå®Ÿéš›ã®åˆ†é¡å¢ƒç•Œã‚’è¦‹ã‚‹ã¨ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã«æ²¿ã£ãŸå¢ƒç•Œã«ã¯ãªã£ã¦ã„ã¾ã›ã‚“ï¼

ã“ã®ã‚ˆã†ã«ï¼Œä¸Šã§æ‰±ã£ãŸ1æ¬¡é–¢æ•°ã§åˆ†é›¢ã™ã‚‹SVMã§ã¯é™ç•ŒãŒã‚ã‚Šã¾ã™ï¼
ã“ã®é™ç•Œã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã«ï¼Œã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã‚’ç”¨ã„ãŸSVMãŒã‚ã‚Šã¾ã™ï¼ã“ã®å®Ÿè£…ã¯æ¬¡å›ã®è¨˜äº‹ã§ç´¹ä»‹ã—ãŸã„ã¨æ€ã„ã¾ã™ï¼
