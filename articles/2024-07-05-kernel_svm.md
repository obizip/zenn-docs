---
title: "ã‚«ãƒ¼ãƒãƒ«SVMã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè£…ã™ã‚‹"
emoji: "ğŸ“"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python", "SVM", "æ©Ÿæ¢°å­¦ç¿’", "æ•°ç†æœ€é©åŒ–"]
published: true
---

## TL;DR
- ç·šå½¢SVMã¨åŒæ§˜ã«ã€ã‚«ãƒ¼ãƒãƒ«SVMã‚’å®Ÿè£…ã—ã¦ã„ã‚‹è¨˜äº‹ã‚‚ã‚ã¾ã‚Šè¦‹ã‹ã‘ãªã‹ã£ãŸã®ã§pythonã§å®Ÿè£…ã—ã¦ã¿ã¾ã—ãŸã€‚
- ã‚«ãƒ¼ãƒãƒ«SVMã®ä¸»å•é¡Œã‚’ã€L2æ­£å‰‡åŒ–çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–å•é¡Œã¨ã—ã¦å‹¾é…é™ä¸‹æ³•ã§è§£ãã¾ã™ã€‚
- è¨ˆç®—é€Ÿåº¦ã¯scikit-learnã®SVCã®ã‚ˆã†ãªå·¥å¤«ã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯åŠã³ã¾ã›ã‚“ã€‚

## æœ¬è¨˜äº‹ãŒå®Ÿè£…ã™ã‚‹SVM
å‰å›ã®ç·šå½¢SVMã®å†…å®¹ã‚’å‰æã¨ã—ã¦é€²ã‚ãŸã„ã¨æ€ã„ã¾ã™ã€‚
2ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’è¡Œã†ã€ã‚«ãƒ¼ãƒãƒ«SVMã®å®Ÿè£…ã‚’è¡Œã„ã¾ã™ã€‚
å®Ÿè£…ã¯ã“ã¡ã‚‰ã«ã‚ã‚Šã¾ã™ã€‚
@[card](https://github.com/obizip/simplesvm)

## å‰å›ã®è¨˜äº‹ã®æŒ¯ã‚Šè¿”ã‚Š
:::message
ç°¡å˜ã®ãŸã‚ã«ç‰¹å¾´é‡$\bm{x}$ã¨é‡ã¿$\bm{w}$ã‚’æ¬¡ã®ã‚ˆã†ã«æ‹¡å¼µã—ã¾ã™ã€‚
$$\tilde{\bm{x}} = [\bm{x}, 1]^\top, \tilde{\bm{w}} = [\bm{w}, b]$$
**ä»¥ä¸‹ã§ã¯å˜ã«$\tilde{\bm{x}}$ã‚’$\bm{x}$ã€$\tilde{\bm{w}}$ã‚’$\bm{w}$ã¨è¡¨è¨˜ã—ã¾ã™ã€‚**
ã¾ãŸã€ç‰¹å¾´é‡ã‚’$\bm{x}_1, \cdots, \bm{x}_n \in \mathbb{R}^d$ã€ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’$y_1, \cdots, y_n \in \{-1, 1\}$ã¨ã—ã¾ã™ã€‚
:::

### ç·šå½¢SVM
ã“ã“ã§ã€ç·šå½¢åˆ†é›¢ä¸å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€$\xi$ã ã‘åˆ†é¡å¢ƒç•Œã‹ã‚‰ã®èª¤å·®ã‚’è¨±å®¹ã™ã‚‹ã‚ˆã†ã«ãƒãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³SVMã‚’ä¸€èˆ¬åŒ–ã—ãŸæ‰‹æ³•ã‚’[ã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVM](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3#%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%BC%E3%82%B8%E3%83%B3)ã¨å‘¼ã³ã¾ã™ã€‚
åŸºæœ¬çš„ãªã‚½ãƒ•ãƒˆãƒãƒ¼ã‚¸ãƒ³SVMã¯$\bm{y}_i \langle \bm{w}, \bm{x_i} \rangle = 0$ã§è¡¨ã•ã‚Œã‚‹ã‚ˆã†ãªä¸€æ¬¡é–¢æ•°ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†é›¢ã™ã‚‹ãŸã‚**ç·šå½¢SVM**ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚

### ä¸»å•é¡Œ

å®šæ•°$\lambda \in \mathbb{R}$ã¨ã—ã¦ã€ç·šå½¢SVMã®ä¸»å•é¡Œã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã›ã¾ã™ã€‚

$$\min_w{\frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\xi_i} \quad \text{s.t.} \quad \xi_i \ge 0, \quad \bm{y}_i \langle \bm{w}, \bm{x_i} \rangle \ge 1 - \xi_i$$

### çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–
ç·šå½¢SVMã®ä¸»å•é¡Œã¯çµŒé¨“ãƒªã‚¹ã‚¯$R$ã‚’ç”¨ã„ã¦æ¬¡ã®ã‚ˆã†ã«ã€ãƒ’ãƒ³ã‚¸æå¤±ã«ã‚ˆã‚‹L2æ­£å‰‡åŒ–çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–å•é¡Œã¨ã—ã¦è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

$$\begin{gather*}\min_w{R(\bm{w})}\\ \text{where} \quad R(\bm{w}) := \frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\max\{0, 1-y_i \langle \bm{w}, \bm{x_i} \rangle \}\end{gather*}$$

### ç·šå½¢SVMã®é™ç•Œ
ã“ã®ç·šå½¢SVMã§ã€æ¬¡ã®ã‚ˆã†ãªmoonãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’äºˆæ¸¬ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã«æ²¿ã£ãŸå¢ƒç•Œã«ã¯ãªã£ã¦ã„ã¾ã›ã‚“ã€‚
ã“ã®ã‚ˆã†ã«ã€ä¸Šã§æ‰±ã£ãŸä¸€æ¬¡é–¢æ•°ã§åˆ†é›¢ã™ã‚‹SVM(ç·šå½¢SVM)ã§ã¯é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚
![dataset_moon](/images/2024-07-04-linear_svm/dataset_moon.png)

## ã‚«ãƒ¼ãƒãƒ«SVMã¨ã¯
ç‰¹å¾´é‡$\bm{x}$ã‚’ç‰¹å¾´å†™åƒ$\Phi$ã«ã‚ˆã£ã¦é«˜æ¬¡å…ƒãªç‰¹å¾´ç©ºé–“ã«å†™ã—ã€ã“ã®é«˜æ¬¡å…ƒç©ºé–“ä¸Šã§ä¸€æ¬¡é–¢æ•°ã«ã‚ˆã‚Šåˆ†é¡ã‚’è¡Œã†æ–¹æ³•ã‚’**ã‚«ãƒ¼ãƒãƒ«SVM**ã¨å‘¼ã³ã¾ã™ã€‚

ç›´æ„Ÿçš„ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’é«˜æ¬¡å…ƒã«å†™ã™ã“ã¨ã§ã€è¡¨ç¾åŠ›ãŒå¢—ã—ã€ã‚ˆã‚Šåˆ†é¡ã—ã‚„ã™ããªã‚Šãã†ã§ã™ã€‚ã—ã‹ã—ã€ç‰¹å¾´ç©ºé–“ã®æ¬¡å…ƒãŒå¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©ã€ãã®ç©ºé–“ã§ã®ç‰¹å¾´é‡$\Phi(\bm{x})$ã®æ¬¡å…ƒã‚‚å¤§ãããªã‚Šã€è¨ˆç®—ã«ã‚³ã‚¹ãƒˆãŒã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã€‚

ã“ã“ã§ã€ç‰¹æ®Šãªé–¢æ•°ã§ã‚ã‚‹**ã‚«ãƒ¼ãƒãƒ«é–¢æ•°**$\kappa$ã¨å‘¼ã°ã‚Œã‚‹2å¤‰æ•°é–¢æ•°ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ç‰¹å¾´ç©ºé–“ä¸­ã®ç‰¹å¾´é‡ã®**æ˜ç¤ºçš„ãªè¨ˆç®—ã‚’çµŒç”±ã›ãš**ã«ã€ç‰¹å¾´ç©ºé–“ã«ãŠã‘ã‚‹**å†…ç©ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥è¨ˆç®—ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™**(ã“ã®ã“ã¨ã‚’**ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯**ã¨ã„ã„ã¾ã™)ã€‚
ä»Šå›ã¯ã€æ¬¡ã®ã‚ˆã†ã«ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ãŒè¡¨ã•ã‚Œã‚‹ã¨ã—ã¾ã™ã€‚
$$\kappa(\bm{u}, \bm{v}) = \left\langle\Phi(\bm{u}), \Phi(\bm{v})\right\rangle$$

## ã‚«ãƒ¼ãƒãƒ«SVMã®çµŒé¨“ãƒªã‚¹ã‚¯é–¢æ•°
ç·šå½¢SVMã®çµŒé¨“ãƒªã‚¹ã‚¯$R$ã¯æ¬¡ã®ã‚ˆã†ã§ã—ãŸã€‚
$$R(\bm{w}) := \frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\max\{0, 1-y_i \langle \bm{w}, \bm{x_i} \rangle \}$$
ä»Šã€ç‰¹å¾´é‡$\bm{x}_i$ã‚’ã€ç‰¹å¾´å†™åƒ$\Phi$ã«ã‚ˆã£ã¦é«˜æ¬¡å…ƒãªç‰¹å¾´ç©ºé–“ã«å†™ã—ãŸ$\Phi(\bm{x}_i)$ã¨ã—ã¦è¨ˆç®—ã™ã‚‹ã“ã¨ã‚’è€ƒãˆã¾ã™ã€‚ã™ã‚‹ã¨ã€ä¸Šå¼ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
$$R(\bm{w}) := \frac{\lambda}{2}||\bm{w}||^2 + \frac{1}{n}\sum^n_{i=1}\max\{0, 1-y_i \langle \bm{w}, \Phi(\bm{x_i}) \rangle\}$$
ã“ã®ã¾ã¾ã€å‰å›ã®ç·šå½¢SVMã¨åŒæ§˜ã«å‹¾é…é™ä¸‹æ³•ã‚’ç”¨ã„ã¦è§£ãã“ã¨ãŒã§ãã‚Œã°ã‚ˆã„ã®ã§ã™ãŒã€ä¸Šã§è¿°ã¹ãŸé€šã‚Šã€é«˜æ¬¡å…ƒç©ºé–“ã§ã®ç‰¹å¾´é‡$\Phi(\bm{x}_i)$ã®è¨ˆç®—ã«ã¯ã€å¤§ããªã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚Šã¾ã™ã€‚

ã“ã“ã§ã€ä¸€èˆ¬çš„ã«ã¯åŒå¯¾å•é¡Œã‚’è€ƒãˆã¾ã™ãŒã€ä»Šå›ã¯åˆ¥ã®æ‰‹æ³•ã‚’ã¨ã‚Šã¾ã™ã€‚
ã“ã®çµŒé¨“ãƒªã‚¹ã‚¯$R$ã®é‡ã¿$\bm{w}$ã¯ã€**ãƒªãƒ—ãƒ¬ã‚¼ãƒ³ã‚¿ãƒ¼å®šç†**ã‚’ç”¨ã„ã¦æ¬¡ã®ã‚ˆã†ã«è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

$\bm{\alpha} = [\alpha_1, ..., \alpha_n] \in \mathbb{R}^n$ã‚’ç”¨ã„ã¦$\bm{w}$ã‚’æ¬¡ã®ã‚ˆã†ã«è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚
$$\bm{w} = \sum^n_{j=1} \alpha_j\Phi(\bm{x}_j)$$
ã“ã‚Œã‚’è¸ã¾ãˆã‚‹ã¨çµŒé¨“ãƒªã‚¹ã‚¯$R$ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™.

ãŸã ã—ã€$K = [\bm{k}_1, ..., \bm{k}_n] = [\kappa(\bm{x}_i, \bm{x}_j)]_{i, j},~\quad i, j \in [n]$ã¨ã—ã¾ã™ã€‚

$$\begin{align*}
	R(\bm{\alpha}) & = \frac{\lambda}{2} \left\|\sum^n_{j=1} \alpha_j\Phi(\bm{x}_j)\right\|^2 + \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1- y_i\left\langle\sum^n_{j=1} \alpha_j\Phi(\bm{x}_j), \Phi(\bm{x}_i)\right\rangle\right\} \\
	           & = \frac{\lambda}{2}\left\|\sum^n_{j=1} \alpha_j\Phi(\bm{x}_j)\right\|^2
	+ \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1-y_i\sum^n_{j=1} \alpha_j\left\langle\Phi(\bm{x}_j), \Phi(\bm{x}_i)\right\rangle \right\} \\
	           & = \frac{\lambda}{2}\left\langle\sum^n_{i=1} \Phi(\bm{x}_i)\alpha_i, \sum^n_{j=1} \Phi(\bm{x}_j)\alpha_j\right\rangle
	+ \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1-y_i \alpha_j \sum^n_{j=1}\left\langle\Phi(\bm{x}_j), \Phi(\bm{x}_i)\right\rangle\right\} \\
	           & = \frac{\lambda}{2}\sum^n_{i=1}\sum^n_{j=1}\alpha_i\left\langle \Phi(\bm{x}_i),  \Phi(\bm{x}_j)\right\rangle\alpha_j
	+ \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1-y_i \alpha_j \sum^n_{j=1}\left\langle\Phi(\bm{x}_j), \Phi(\bm{x}_i)\right\rangle\right\} \\
	           & = \frac{\lambda}{2}\sum^n_{i=1}\sum^n_{j=1}\alpha_i\kappa(\bm{x}_i, \bm{x}_j)\alpha_j
	+ \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1-y_i \alpha_j \sum^n_{j=1}\kappa(\bm{x}_j, \bm{x}_i)\right\}          \\
	           & = \frac{\lambda}{2}\left\langle\bm{\alpha}, K \bm{\alpha}\right\rangle
	+ \frac{1}{n}\sum^n_{i=1} \max\left\{0, 1-y_i\left\langle\bm{\alpha}, \bm{k}_i\right\rangle\right\}
\end{align*}$$

ã‚ˆã£ã¦çµŒé¨“ãƒªã‚¹ã‚¯$R$ã®å‹¾é…ã¯è¨ˆç®—ã§ãã¾ã™ã€‚

$$\begin{align*}
    \nabla R(\bm{\alpha}) & = \lambda K \bm{\alpha}
    + \frac{1}{n}\sum^n_{i=1} \begin{cases} -y_i\bm{k}_i &\text{if } 1-y_i \langle \bm{\alpha}, \bm{k}_i \rangle \ge 0 \\ 0 &\text{otherwise}\end{cases} \\
\end{align*}$$

### çµŒé¨“ãƒªã‚¹ã‚¯æœ€å°åŒ–ã¨æœ€æ€¥é™ä¸‹æ³•
ç·šå½¢SVMã¨åŒæ§˜ã«ã€çµŒé¨“ãƒªã‚¹ã‚¯$R$ãŒ[å‡¸é–¢æ•°](https://ja.wikipedia.org/wiki/%E5%87%B8%E9%96%A2%E6%95%B0)ã§ã‚ã‚‹ã“ã¨ã‹ã‚‰ã€å˜ç´”ã«[æœ€æ€¥é™ä¸‹æ³•](https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95)ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æœ€é©è§£ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ã¤ã¾ã‚Šã€$t$ã‚¹ãƒ†ãƒƒãƒ—ç›®ã®é‡ã¿$\bm{\alpha}^{(t)}$ã¯ã€å­¦ç¿’ç‡$\eta \in \mathbb{R}$ã‚’å®šã‚ã¦ã€å‰ã‚¹ãƒ†ãƒƒãƒ—ã®é‡ã¿$\bm{\alpha}^{(t-1)}$ã¨ã€çµŒé¨“ãƒªã‚¹ã‚¯$R$ã®å‹¾é…ã‹ã‚‰ã€æ¬¡ã®ã‚ˆã†ã«æ±ºã‚ã¦ã„ã‘ã°è‰¯ã„ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
$$\bm{\alpha}^{(t)} = \bm{\alpha}^{(t-1)} - \eta \nabla R(\bm{\alpha}^{(t-1)})$$
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ååˆ†ã«ç¹°ã‚Šè¿”ã™ã“ã¨ã§ã€é‡ã¿$\bm{\alpha}$ã¯çµŒé¨“ãƒªã‚¹ã‚¯$R$ã‚’æœ€å°ã¨ã™ã‚‹æœ€é©è§£ã«è¿‘ã¥ã„ã¦ã„ãã¾ã™ã€‚

### ã‚«ãƒ¼ãƒãƒ«SVMã§ã®äºˆæ¸¬
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿$\tilde{\bm{x}}_1, \cdots, \tilde{\bm{x}}_m \in \mathbb{R}^d$ã«å¯¾ã—ã¦ã€å‹¾é…é™ä¸‹æ³•ã§å¾—ãŸ$\alpha$ã‚’ç”¨ã„ã¦äºˆæ¸¬å€¤ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯æ¬¡ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚ ãŸã ã—ã€$\tilde{K} = [\tilde{\bm{k}}_1, ..., \tilde{\bm{k}}_m] = [\kappa(\bm{x}_i, \tilde{\bm{x}}_j)]_{i, j}, \quad (i, j) \in [n]\times[m]$ã¨ã—ã¾ã™ã€‚

$$\begin{align*}
	i \in [m], \quad \hat{\bm{y}} & = [\left\langle\bm{w}, \Phi(\tilde{\bm{x}}_i)\right\rangle]_i                                                            \\
	                          & = \left[\left\langle\sum^n_{j=1}\alpha_j \Phi(\bm{x}_j) , \Phi(\tilde{\bm{x}}_i)\right\rangle\right]_i \\
	                          & = \left[\sum^n_{j=1} \alpha_j\left\langle\Phi(\bm{x}_j), \Phi(\tilde{\bm{x}}_i)\right\rangle \right]_i \\
	                          & = \left[\left\langle\bm{\alpha}, \tilde{\bm{k}}_i\right\rangle\right]_i                               \\
\end{align*}$$

## ã‚«ãƒ¼ãƒãƒ«SVMã®å®Ÿè£…
ã“ã“ã§ã¯ã€[RBFã‚«ãƒ¼ãƒãƒ«](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)ã‚’ç”¨ã„ã¾ã—ãŸã€‚ã“ã®ã‚«ãƒ¼ãƒãƒ«ã«ãŠã‘ã‚‹ç‰¹å¾´å†™åƒ$\Phi$ã¯ç„¡é™æ¬¡å…ƒç©ºé–“ã«å†™åƒã™ã‚‹ãŸã‚ã€è¡¨ç¾åŠ›ãŒã¨ã¦ã‚‚é«˜ã„ã§ã™ã€‚
æ¬¡ã®ã‚ˆã†ã«scikit-learnã®APIã«å¯„ã›ã¦å®Ÿè£…ã—ã¾ã—ãŸã€‚
```python
import numpy as np


def _rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0):
    """Compute RBF kernel (Gaussian kernel).

    Parameters
    ----------
    X : numpy.ndarray
        First feature array.
    Y : numpy.ndarray
        Second feature array.
    gamma : float, default=1.0
        Parameter of the RBF kernel.

    Returns
    -------
    K : numpy.ndarray
        The RBF kernel.
    """
    # Compute the squared Euclidean distance between each pair of points in X and Y
    # ||x-y||^2 = ||x||^2 + ||y||^2 - 2 * x^T * y
    X_norm = np.sum(X**2, axis=1).reshape(-1, 1)
    Y_norm = np.sum(Y**2, axis=1).reshape(1, -1)
    D_square = X_norm + Y_norm - 2 * np.dot(X, Y.T)

    # Apply the RBF kernel formula
    K = np.exp(-gamma * D_square)

    return K


class KernelSVM:
    def __init__(
        self,
        lam: float = 0.01,
        n_iters: int = 5000,
        learning_rate: float = 0.01,
        kernel: str = "rbf",
        gamma: float = 1.0,
        bias=True,
        verbose=False,
    ):
        """Kernel Support Vector Machine (SVM) for Binary Classification.

        Parameters
        ----------
        lam : float, default=0.01
            Regularization parameter.
        n_iters : int, default=5000
            Number of iterations for the gradient descent.
        learning_rate : float, default=0.01
            Learning rate for the gradient descent.
        kernel : str, default="rbf"
            Kernel type to be used in the algorithm. It must be one of 'linear', 'rbf'.
        gamma : float, default=1.0
            Kernel parameter for 'rbf'.
        bias : bool, default=True
            Whether to include a bias term in the input matrix.
        verbose : bool, default=False
            If True, print progress messages during the gradient descent.
        """
        self.lam = lam
        self.n_iters = n_iters
        self.learning_rate = learning_rate
        self.bias = bias
        self.kernel = kernel
        self.gamma = gamma
        self.verbose = verbose
        self._Xfit = None
        self._classes = {}
        self._a = None

    # Calculate kernel function
    def _kernelize(self, X, Y):
        if self.kernel == "linear":
            return X @ Y.T
        elif self.kernel == "rbf":
            return _rbf_kernel(X, Y, gamma=self.gamma)
        else:
            raise ValueError(f"Unexpected kernel type: {self.kernel}")

    # Calculate the empirical risk function.
    def _empirical_risk(
        self, K: np.ndarray, y: np.ndarray, a_t: np.ndarray
    ) -> np.ndarray:
        regularzation = 0.5 * self.lam * (a_t @ K @ a_t)
        loss = np.where(1 - y * (K @ a_t) >= 0, 1 - y * (K @ a_t, 0)).mean()

        return regularzation + loss

    # Calculate the gradient of the empirical risk function.
    def _empirical_risk_grad(
        self, K: np.ndarray, y: np.ndarray, a_t: np.ndarray
    ) -> np.ndarray:
        regularzation_grad = self.lam * (K @ a_t)
        loss_grad = (
            (np.where(1 - y * (K @ a_t) >= 0, 1, 0) * -y).reshape(-1, 1) * K
        ).mean(axis=0)

        return regularzation_grad + loss_grad

    def fit(self, X: np.ndarray, y: np.ndarray):
        """Fit the Kernel SVM model according to the given training data.

        Parameters
        ----------
        X : numpy.ndarray of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples
            and `n_features` is the number of features.

        y : numpy.ndarray of shape (n_samples,)
            Class labels in classification.

        Returns
        -------
        self : object
            Fitted estimator.
        """
        # validate and change class labels
        classes = np.unique(y)
        if len(classes) != 2:
            raise ValueError("Class labels is not binary")
        self._classes[-1] = classes[0]
        self._classes[1] = classes[1]
        y = np.where(y == self._classes[1], 1, -1)

        if self.bias:
            X = np.c_[X, np.ones(X.shape[0])]

        self._Xfit = X
        K = self._kernelize(X, X)

        # alpha for weights
        a = np.ones(K.shape[1])

        # gradient descent
        for i in range(self.n_iters):
            a = a - self.learning_rate * self._empirical_risk_grad(K, y, a)
            if self.verbose and (i + 1) % 1000 == 0:
                print(f"{i+1:4}: R(a) = {self._empirical_risk(K, y, a)}")
        self._a = a

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        """Evaluate the decision function for the samples in X.

        Parameters
        ----------
        X : numpy.ndarray of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        y_score : ndarray of shape (n_samples, )
            Returns the decision function of the sample for each class in the model.
            The decision function is calculated based on the class labels 1 and -1.
        """
        if self.bias:
            X = np.c_[X, np.ones(X.shape[0])]

        K = self._kernelize(X, self._Xfit)
        y_score = K @ self._a

        return y_score

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Perform classification on samples in X.

        Parameters
        ----------
        X : numpy.ndarray of shape (n_samples, n_features)

        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
            Class labels for samples in X.
        """
        y_score = self.decision_function(X)
        y_pred = np.where(y_score > 0, 1, -1)
        y_pred = np.where(y_pred == 1, self._classes[1], self._classes[-1])

        return y_pred

```

ã•ã¦ã€ã“ã®ã‚«ãƒ¼ãƒãƒ«SVMã‚’moonãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ç”¨ã„ã‚‹ã¨æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
```python
import numpy as np
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

np.random.seed(42)

X, y = make_moons(n_samples=500, noise=0.1, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y)

model = KernelSVM()
model.fit(X_train, y_train)
preds = model.predict(X_test)

print(f"ACC: {accuracy_score(y_test, preds)}")
#> ACC: 0.992

# Plot a decision boundary
x_min=X[:, 0].min() - 0.5
x_max=X[:, 0].max() + 0.5
y_min=X[:, 1].min() - 0.5
y_max=X[:, 1].max() + 0.5

xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))
XY = np.array([xx.ravel(), yy.ravel()]).T
z = model.predict(XY)
plt.contourf(xx, yy, z.reshape(xx.shape), alpha=0.2, cmap=plt.cm.coolwarm)
plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=plt.cm.coolwarm)
plt.show()
```
ACCã¯0.992ã¨ãªã‚Šã€è­˜åˆ¥å¢ƒç•Œã‚‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆã£ãŸå½¢ã¨ãªã£ã¦ã„ã¾ã™ã€‚
![plot_boundary2](/images/2024-07-05-kernel_svm/plot_boundary.png)
